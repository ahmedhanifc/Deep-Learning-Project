{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbd0fe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (2.9.1)\n",
      "Requirement already satisfied: torchvision in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (0.24.1)\n",
      "Requirement already satisfied: matplotlib in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (3.10.7)\n",
      "Requirement already satisfied: pandas in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (2.3.5)\n",
      "Requirement already satisfied: seaborn in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (1.7.2)\n",
      "Requirement already satisfied: filelock in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ahmedhanif/miniconda3/envs/ds1/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision matplotlib pandas numpy seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a3bc8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from src.data import WasteClassificationDataset\n",
    "from src.helper_functions import plot_random_image_and_label, print_model_parameter_breakdown, save_model, save_config_to_json, save_run_to_csv\n",
    "from src.model import NeuralNetwork, train_model, evaluate_model\n",
    "from src.plots import plot_loss_curves, plot_accuracy_curves, plot_confusion_matrix, plot_combined_metrics, plot_f1_curves, plot_precision_curves, plot_recall_curves\n",
    "from src.metrics import print_metrics_summary, print_classification_report\n",
    "from src.config import TRANSFORM_CONFIG, MODEL_CONFIG, MODEL_ARCHITECTURE_CONFIG\n",
    "from src.helper_functions import EarlyStopping\n",
    "from src.config import calculate_input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5957881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a8fa8",
   "metadata": {},
   "source": [
    "# Creating Custom Dataset:\n",
    "\n",
    "A custom Dataset class must implement three functions: __init__, __len__, and __getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ae022b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transforms = transforms.Compose([\n",
    "#     transforms.Resize(TRANSFORM_CONFIG[\"image_size\"]),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(\n",
    "#         mean=TRANSFORM_CONFIG[\"normalize_mean\"], \n",
    "#         std=TRANSFORM_CONFIG[\"normalize_std\"]\n",
    "#     )\n",
    "# ])\n",
    "\n",
    "# test_transforms = transforms.Compose([\n",
    "#     transforms.Resize(TRANSFORM_CONFIG[\"image_size\"]),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(\n",
    "#         mean=TRANSFORM_CONFIG[\"normalize_mean\"], \n",
    "#         std=TRANSFORM_CONFIG[\"normalize_std\"]\n",
    "#     )\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d307749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = WasteClassificationDataset(\"./data/train\", transform=train_transforms)\n",
    "# test_dataset = WasteClassificationDataset(\"./data/test\", transform=test_transforms)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=MODEL_CONFIG[\"batch_size\"], shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=MODEL_CONFIG[\"batch_size\"], shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "807f5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_random_image_and_label(\n",
    "#     train_dataset, \n",
    "#     train_dataset.classes,\n",
    "#     mean=TRANSFORM_CONFIG[\"normalize_mean\"],\n",
    "#     std=TRANSFORM_CONFIG[\"normalize_std\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55392220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot sample images from each class using the plot_stuff method\n",
    "# train_dataset.plot_stuff(num_samples_per_class=4, show_transformed=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e1c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6e28caa",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f816815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = [label for _, label in train_dataset]\n",
    "# class_counts = Counter(labels)\n",
    "# class_names = train_dataset.classes\n",
    "# counts = {class_names[k]: v for k, v in class_counts.items()}\n",
    "# counts\n",
    "# # no class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f9c2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_average_images(dataset, classes):\n",
    "#     # Initialize sums\n",
    "#     # Assuming images are 224x224 based on your config\n",
    "#     class_sums = {c: np.zeros((224, 224, 3)) for c in classes}\n",
    "#     class_counts = {c: 0 for c in classes}\n",
    "\n",
    "#     print(\"Calculating average images... this might take a minute.\")\n",
    "    \n",
    "#     for img, label_idx in dataset:\n",
    "#         # Image comes in as Tensor (C, H, W) -> Convert to numpy (H, W, C)\n",
    "#         img = img.permute(1, 2, 0).numpy()\n",
    "        \n",
    "#         # If your images are already normalized (mean=0, std=1), \n",
    "#         # they might look weird. Un-normalize them for visualization if needed.\n",
    "#         # For just checking structure, raw values are fine.\n",
    "        \n",
    "#         label_name = classes[label_idx]\n",
    "#         class_sums[label_name] += img\n",
    "#         class_counts[label_name] += 1\n",
    "\n",
    "#     plt.figure(figsize=(15, 5))\n",
    "#     for i, class_name in enumerate(classes):\n",
    "#         if class_counts[class_name] > 0:\n",
    "#             avg_img = class_sums[class_name] / class_counts[class_name]\n",
    "            \n",
    "#             # Normalize to 0-1 for display\n",
    "#             avg_img = (avg_img - avg_img.min()) / (avg_img.max() - avg_img.min())\n",
    "            \n",
    "#             plt.subplot(1, 4, i+1)\n",
    "#             plt.imshow(avg_img)\n",
    "#             plt.title(f\"Average {class_name}\")\n",
    "#             plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# # Run the function\n",
    "# plot_average_images(train_dataset, train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5236fb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "\n",
    "# def visualize_embeddings(dataset, num_samples=500):\n",
    "#     print(\"Extracting data for t-SNE...\")\n",
    "#     images = []\n",
    "#     labels = []\n",
    "    \n",
    "#     # Get a subset to save time\n",
    "#     indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "#     for idx in indices:\n",
    "#         img, label = dataset[idx]\n",
    "#         # Flatten the image: (3, 224, 224) -> 150528\n",
    "#         images.append(img.numpy().flatten())\n",
    "#         labels.append(dataset.classes[label])\n",
    "    \n",
    "#     # Convert to numpy array\n",
    "#     X = np.array(images)\n",
    "    \n",
    "#     # Run t-SNE\n",
    "#     print(\"Running t-SNE (this converts 150k pixels -> 2 coordinates)...\")\n",
    "#     tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
    "#     X_embedded = tsne.fit_transform(X)\n",
    "    \n",
    "#     # Plot\n",
    "#     df = pd.DataFrame(X_embedded, columns=['x', 'y'])\n",
    "#     df['label'] = labels\n",
    "    \n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     sns.scatterplot(data=df, x='x', y='y', hue='label', palette='tab10', alpha=0.7)\n",
    "#     plt.title(\"t-SNE Projection of Raw Images\")\n",
    "#     plt.show()\n",
    "\n",
    "# # Run it (might take 1-2 mins)\n",
    "# visualize_embeddings(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c1619",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- summarize your work, clearly stating the project's objectives. \n",
    "\n",
    "<div style = \"color:blue\">\n",
    "    We are given a Waste Dataset, divided into training and test set with each having for class labels (glass, metal, paper, and plastic) and our goal is to develop a Deep Neural Network to classify images correctly into one of these types. \n",
    "    For this project, I will be using PyTorch for my Implementation\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99724178",
   "metadata": {},
   "source": [
    "# Deep Learning Architecture:\n",
    "\n",
    "Explain your Deep Learning Architecture: What kind of Deep Learning architecture did you use in your project, and why? Number of layers? Number of hidden units? etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a43adf2",
   "metadata": {},
   "source": [
    "<div style = \"color:purple\">\n",
    "https://medium.com/@sahin.samia/mastering-the-basics-of-torch-nn-a-comprehensive-guide-to-pytorchs-neural-network-module-9f2d704e8c7f <br>\n",
    "\n",
    "torch.nn = It provides a rich collection of tools to define, manage, and manipulate deep learning models efficiently. With torch.nn, you can construct everything from simple feedforward networks to highly complex architectures like convolutional and recurrent neural networks. <br>\n",
    "\n",
    "Every model in PyTorch is essentially a subclass of nn.Module, making it the essential blueprint for neural network design. It provides:\n",
    "\n",
    "Initialization (__init__): Define the layers and components of your network. <br>\n",
    "Forward Pass (forward): Specify how data flows through the layers of your network. <br>\n",
    "Parameter Management: Automatically tracks and optimizes model parameters.<br>\n",
    "\n",
    "<br>\n",
    "- Common Layers in nn <br>\n",
    "\n",
    "1. nn.Linear: Fully Connected Layer <br>\n",
    "Purpose: Applies a linear transformation: y = xW^T + b <br>\n",
    "Common Use: Used in feedforward networks for dense connections between layers.<br>\n",
    "\n",
    "2. nn.ReLU: Activation Function <br>\n",
    "Purpose: Applies the ReLU function: ReLU(x)=max⁡(0,x) <br>\n",
    "Common Use: Adds non-linearity after linear <br>\n",
    "\n",
    "3. nn.Dropout: Dropout Regularization <br>\n",
    "Purpose: Randomly sets a fraction of input neurons to zero during training to prevent overfitting. <br>\n",
    "Common Use: Used after fully connected layers or in deeper architectures. <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "335fa650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image, label = train_dataset[0]\n",
    "# image.shape # need to flatten the image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9766cd8",
   "metadata": {},
   "source": [
    "# Loss function:\n",
    "\n",
    "Which loss function did you select to use and why?\n",
    "\n",
    "\n",
    "Plot the change in your loss function (both training loss and test loss) vs epoch. Your code shall generate the “Change of Training and Test losses vs Epoch’ graph\n",
    "\n",
    "Discuss these loss curves\n",
    "\n",
    "<div style = \"color:purple\">\n",
    "    Used to quantify the difference between predicted and actual. <br><br>\n",
    "    Common Activation Functions <br>\n",
    "    1. nn.MSELoss: Mean Squared Error Loss<br>\n",
    "    Purpose: Measures the average squared difference between the predicted values and the actual target values. <br>\n",
    "    Common Use: Used in regression tasks where the goal is to predict continuous values. <br><br>\n",
    "    2. nn.CrossEntropyLoss: Cross-Entropy Loss <br>\n",
    "    Purpose: Combines nn.LogSoftmax and nn.NLLLoss to calculate the cross-entropy between predicted probabilities and true class labels. <br>\n",
    "    Common Use: Used in multi-class classification tasks. <br><br>\n",
    "    3. nn.BCELoss: Binary Cross-Entropy for binary classification problems. <br><br>\n",
    "    4. nn.BCEWithLogitsLoss: Combines a Sigmoid activation with Binary Cross-Entropy, more numerically stable than using nn.BCELoss with a separate Sigmoid.<br><br>\n",
    "    5. nn.HingeEmbeddingLoss: Useful for tasks like similarity learning or when using SVM-like objectives.<br><br>\n",
    "    6. nn.SmoothL1Loss: Combines MSE and L1 losses to handle outliers more gracefully.<br><br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27258a21",
   "metadata": {},
   "source": [
    "<div style = \"color:darkblue\">\n",
    "    Optimizers and torch.optim Integration <br><br>\n",
    "    Models built with torch.nn automatically track their trainable parameters (e.g., weights and biases). <br>\n",
    "    These parameters are passed to an optimizer from torch.optim to compute and apply parameter updates. <br><br>\n",
    "    torch.nn defines the structure and forward computation of the model. <br>\n",
    "    torch.optim takes care of adjusting the model's parameters during training based on the gradients computed by backpropagation. <br><br>\n",
    "    torch.nn computes the forward pass and loss.<br>\n",
    "    Gradients are computed via loss.backward().<br>\n",
    "    torch.optim updates parameters using optimizer.step().<br><br>\n",
    "    1. Stochastic Gradient Descent (SGD)<br>\n",
    "        Description: Updates parameters by moving them in the direction of the negative gradient of the loss function.<br>\n",
    "        Configuration:<br>\n",
    "        lr (learning rate): Controls the step size for updates.<br>\n",
    "        momentum: Adds inertia to the updates, helping to overcome small local minima.<br><br>\n",
    "    2.  Adam Optimizer <br>\n",
    "        Description: Combines the benefits of SGD with momentum and adaptive learning rates. <br>\n",
    "        Configuration: <br>\n",
    "        lr: Learning rate.<br>\n",
    "        betas: Coefficients for computing moving averages of gradient and its square (default: (0.9, 0.999)).<br>\n",
    "        eps: Small value to prevent division by zero.<br><br>\n",
    "    3. RMSprop<br>\n",
    "        Description: Similar to Adam but uses only squared gradients to adjust learning rates.<br>\n",
    "        Common Use: Suitable for recurrent neural networks.<br><br>\n",
    "    4. Adagrad: Adjusts learning rates for individual parameters based on their updates.<br><br>\n",
    "    5. AdamW: A variant of Adam with decoupled weight decay for better generalization.<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8d4c16",
   "metadata": {},
   "source": [
    "<img src=\"./images/hyperparametersopt.png\" alt=\"Machine Learning Recipe\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230dfd3a",
   "metadata": {},
   "source": [
    "<img >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544caad8",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "<img src = \"./images/hyperparameters.png\">\n",
    "\n",
    "\n",
    "The number of layers\n",
    "\n",
    "\n",
    "Number of nodes at each layer\n",
    "\n",
    "\n",
    "Learning rate\n",
    "\n",
    "<div style = \"blue\">\n",
    "We shoud slowly reduce alpha\n",
    "\n",
    "<img src = \"./images/lr1.png\">\n",
    "<img src = \"./images/lr2.png\">\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "“Mini batch” size\n",
    "- What is the mini-batch size you used in your code?\n",
    "\n",
    "\n",
    "- Why did you select this value?\n",
    "\n",
    "\n",
    "\n",
    "Activation functions\n",
    "- What is the activation function(s) used in your code?\n",
    "\n",
    "\n",
    "\n",
    "- Why did you select them to use?\n",
    "\n",
    "\n",
    "\n",
    "Optimization function\n",
    "- Which optimization function did you use? Why?\n",
    "\n",
    "\n",
    "\n",
    "- What are the optimization parameters used?\n",
    "\n",
    "\n",
    "Regularization function \n",
    "- Did you use any regularization function?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Why used/ not used?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- If used, what are the regularization parameters?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Epochs\n",
    "- What is the number of epochs used?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Why did you select this value?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea77efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'hyperparameters.json'\n",
    "with open(config_path, 'r') as f:\n",
    "    hyperparameter_configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcb57fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting run_1_20251129_193314\n",
      "Run directory: runs/run_1_20251129_193314\n",
      "Config: {'learning_rate': 0.0005, 'batch_size': 32, 'epochs': 60, 'hidden_sizes': [512, 256, 128], 'dropout_rates': [0.4, 0.4, 0.3], 'weight_decay': 0.01, 'notes': 'Lower Dropout and Higher Hidden Size', 'image_size': [64, 64], 'early_stopping': {'patience': 10, 'min_delta': 0.0009, 'mode': 'min', 'restore_best_weights': True}}\n",
      "============================================================\n",
      "\n",
      "[Dataset] Loaded 1540 examples from 4 classes: ['glass', 'metal', 'paper', 'plastic']\n",
      "[Dataset] Loaded 100 examples from 4 classes: ['glass', 'metal', 'paper', 'plastic']\n",
      "Epoch [1/60] - LR: 0.000500\n",
      "  Train - Loss: 1.5001, Acc: 0.2857, F1: 0.2837\n",
      "  Test  - Loss: 1.2704, Acc: 0.4700, F1: 0.4403\n",
      "  Early Stop Counter: 0/10\n",
      "--------------------------------------------------\n",
      "Epoch [2/60] - LR: 0.000500\n",
      "  Train - Loss: 1.3572, Acc: 0.3519, F1: 0.3497\n",
      "  Test  - Loss: 1.2395, Acc: 0.3800, F1: 0.3230\n",
      "  Early Stop Counter: 0/10\n",
      "--------------------------------------------------\n",
      "Epoch [3/60] - LR: 0.000500\n",
      "  Train - Loss: 1.2666, Acc: 0.4130, F1: 0.4120\n",
      "  Test  - Loss: 1.3169, Acc: 0.4400, F1: 0.3542\n",
      "  Early Stop Counter: 1/10\n",
      "--------------------------------------------------\n",
      "Epoch [4/60] - LR: 0.000500\n",
      "  Train - Loss: 1.2413, Acc: 0.4370, F1: 0.4359\n",
      "  Test  - Loss: 1.1796, Acc: 0.4800, F1: 0.4271\n",
      "  Early Stop Counter: 0/10\n",
      "--------------------------------------------------\n",
      "Epoch [5/60] - LR: 0.000500\n",
      "  Train - Loss: 1.2250, Acc: 0.4494, F1: 0.4435\n",
      "  Test  - Loss: 1.1334, Acc: 0.5900, F1: 0.5738\n",
      "  Early Stop Counter: 0/10\n",
      "--------------------------------------------------\n",
      "Epoch [6/60] - LR: 0.000500\n",
      "  Train - Loss: 1.1904, Acc: 0.4636, F1: 0.4623\n",
      "  Test  - Loss: 1.1443, Acc: 0.5400, F1: 0.5270\n",
      "  Early Stop Counter: 1/10\n",
      "--------------------------------------------------\n",
      "Epoch [7/60] - LR: 0.000500\n",
      "  Train - Loss: 1.1416, Acc: 0.4961, F1: 0.4939\n",
      "  Test  - Loss: 1.1153, Acc: 0.5200, F1: 0.4896\n",
      "  Early Stop Counter: 0/10\n",
      "--------------------------------------------------\n",
      "Epoch [8/60] - LR: 0.000500\n",
      "  Train - Loss: 1.1058, Acc: 0.5279, F1: 0.5245\n",
      "  Test  - Loss: 1.0104, Acc: 0.5700, F1: 0.5617\n",
      "  Early Stop Counter: 0/10\n",
      "--------------------------------------------------\n",
      "Epoch [9/60] - LR: 0.000500\n",
      "  Train - Loss: 1.0425, Acc: 0.5526, F1: 0.5534\n",
      "  Test  - Loss: 1.0596, Acc: 0.5600, F1: 0.5177\n",
      "  Early Stop Counter: 1/10\n",
      "--------------------------------------------------\n",
      "Epoch [10/60] - LR: 0.000500\n",
      "  Train - Loss: 1.0229, Acc: 0.5669, F1: 0.5659\n",
      "  Test  - Loss: 1.0319, Acc: 0.5400, F1: 0.5365\n",
      "  Early Stop Counter: 2/10\n",
      "--------------------------------------------------\n",
      "Epoch [11/60] - LR: 0.000500\n",
      "  Train - Loss: 0.9900, Acc: 0.5526, F1: 0.5505\n",
      "  Test  - Loss: 1.1203, Acc: 0.5600, F1: 0.5239\n",
      "  Early Stop Counter: 3/10\n",
      "--------------------------------------------------\n",
      "Epoch [12/60] - LR: 0.000500\n",
      "  Train - Loss: 0.9505, Acc: 0.6052, F1: 0.6040\n",
      "  Test  - Loss: 1.0161, Acc: 0.5600, F1: 0.5323\n",
      "  Early Stop Counter: 4/10\n",
      "--------------------------------------------------\n",
      "Epoch [13/60] - LR: 0.000500\n",
      "  Train - Loss: 0.9870, Acc: 0.6000, F1: 0.5985\n",
      "  Test  - Loss: 0.9941, Acc: 0.5800, F1: 0.5551\n",
      "  Early Stop Counter: 0/10\n",
      "--------------------------------------------------\n",
      "Epoch [14/60] - LR: 0.000500\n",
      "  Train - Loss: 0.9363, Acc: 0.6084, F1: 0.6084\n",
      "  Test  - Loss: 0.9650, Acc: 0.6300, F1: 0.6225\n",
      "  Early Stop Counter: 0/10\n",
      "--------------------------------------------------\n",
      "Epoch [15/60] - LR: 0.000500\n",
      "  Train - Loss: 0.9092, Acc: 0.6182, F1: 0.6178\n",
      "  Test  - Loss: 1.1613, Acc: 0.5000, F1: 0.4907\n",
      "  Early Stop Counter: 1/10\n",
      "--------------------------------------------------\n",
      "Epoch [16/60] - LR: 0.000500\n",
      "  Train - Loss: 0.8668, Acc: 0.6227, F1: 0.6212\n",
      "  Test  - Loss: 1.0337, Acc: 0.5800, F1: 0.5795\n",
      "  Early Stop Counter: 2/10\n",
      "--------------------------------------------------\n",
      "Epoch [17/60] - LR: 0.000500\n",
      "  Train - Loss: 0.8591, Acc: 0.6455, F1: 0.6458\n",
      "  Test  - Loss: 1.0345, Acc: 0.6100, F1: 0.6076\n",
      "  Early Stop Counter: 3/10\n",
      "--------------------------------------------------\n",
      "Epoch [18/60] - LR: 0.000500\n",
      "  Train - Loss: 0.8315, Acc: 0.6396, F1: 0.6400\n",
      "  Test  - Loss: 1.0712, Acc: 0.6000, F1: 0.5916\n",
      "  Early Stop Counter: 4/10\n",
      "--------------------------------------------------\n",
      "Epoch [19/60] - LR: 0.000500\n",
      "  Train - Loss: 0.7989, Acc: 0.6792, F1: 0.6800\n",
      "  Test  - Loss: 1.1942, Acc: 0.6200, F1: 0.6168\n",
      "  Early Stop Counter: 5/10\n",
      "--------------------------------------------------\n",
      "Epoch [20/60] - LR: 0.000500\n",
      "  Train - Loss: 0.8112, Acc: 0.6688, F1: 0.6674\n",
      "  Test  - Loss: 1.1016, Acc: 0.6500, F1: 0.6511\n",
      "  Early Stop Counter: 6/10\n",
      "--------------------------------------------------\n",
      "Epoch [21/60] - LR: 0.000250\n",
      "  Train - Loss: 0.7460, Acc: 0.6857, F1: 0.6849\n",
      "  Test  - Loss: 1.0985, Acc: 0.6000, F1: 0.5969\n",
      "  Early Stop Counter: 7/10\n",
      "--------------------------------------------------\n",
      "Epoch [22/60] - LR: 0.000250\n",
      "  Train - Loss: 0.7001, Acc: 0.7045, F1: 0.7050\n",
      "  Test  - Loss: 1.0961, Acc: 0.6200, F1: 0.6143\n",
      "  Early Stop Counter: 8/10\n",
      "--------------------------------------------------\n",
      "Epoch [23/60] - LR: 0.000250\n",
      "  Train - Loss: 0.6553, Acc: 0.7351, F1: 0.7354\n",
      "  Test  - Loss: 1.1180, Acc: 0.6500, F1: 0.6494\n",
      "  Early Stop Counter: 9/10\n",
      "--------------------------------------------------\n",
      "\n",
      "Early stopping triggered at epoch 24\n",
      "Best validation loss: 0.9650\n",
      "Restored best model weights\n",
      "Plot saved to runs/run_1_20251129_193314/loss_curve.png\n",
      "Plot saved to runs/run_1_20251129_193314/accuracy_curve.png\n",
      "Plot saved to runs/run_1_20251129_193314/f1_curve.png\n",
      "Plot saved to runs/run_1_20251129_193314/precision_curve.png\n",
      "Plot saved to runs/run_1_20251129_193314/recall_curve.png\n",
      "Plot saved to runs/run_1_20251129_193314/confusion_matrix.png\n",
      "Saved model to runs/run_1_20251129_193314/model.pth\n",
      "Saved config to runs/run_1_20251129_193314/config.json\n",
      "\n",
      "*** New best model! Test Accuracy: 0.6400 ***\n",
      "\n",
      "Run run_1_20251129_193314 completed in 67.55 seconds\n",
      "Test Accuracy: 0.6400\n",
      "All artifacts saved to: runs/run_1_20251129_193314\n",
      "\n",
      "\n",
      "============================================================\n",
      "Hyperparameter search complete!\n",
      "Best run: run_1_20251129_193314\n",
      "Best run directory: runs/run_1_20251129_193314\n",
      "Best test accuracy: 0.6400\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "best_test_accuracy = 0.0\n",
    "best_run_id = None\n",
    "best_run_dir = None\n",
    "RUNS_BASE_DIR = \"runs\"\n",
    "os.makedirs(RUNS_BASE_DIR, exist_ok=True)\n",
    "for run_idx, hp_config in enumerate(hyperparameter_configs):\n",
    "    # Create unique run ID and directory\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    run_id = f\"run_{run_idx+1}_{timestamp}\"\n",
    "    run_dir = os.path.join(RUNS_BASE_DIR, run_id)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting {run_id}\")\n",
    "    print(f\"Run directory: {run_dir}\")\n",
    "    print(f\"Config: {hp_config}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Update configs\n",
    "    MODEL_CONFIG[\"learning_rate\"] = hp_config[\"learning_rate\"]\n",
    "    MODEL_CONFIG[\"batch_size\"] = hp_config[\"batch_size\"]\n",
    "    MODEL_CONFIG[\"epochs\"] = hp_config[\"epochs\"]\n",
    "    MODEL_CONFIG[\"weight_decay\"] = hp_config[\"weight_decay\"]\n",
    "    MODEL_ARCHITECTURE_CONFIG[\"hidden_sizes\"] = hp_config[\"hidden_sizes\"]\n",
    "    MODEL_ARCHITECTURE_CONFIG[\"dropout_rates\"] = hp_config[\"dropout_rates\"]\n",
    "    TRANSFORM_CONFIG[\"image_size\"] = tuple(hp_config[\"image_size\"])\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize(TRANSFORM_CONFIG[\"image_size\"]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=TRANSFORM_CONFIG[\"normalize_mean\"], \n",
    "            std=TRANSFORM_CONFIG[\"normalize_std\"]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.Resize(TRANSFORM_CONFIG[\"image_size\"]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=TRANSFORM_CONFIG[\"normalize_mean\"], \n",
    "            std=TRANSFORM_CONFIG[\"normalize_std\"]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    train_dataset = WasteClassificationDataset(\"./data/train\", transform=train_transforms)\n",
    "    test_dataset = WasteClassificationDataset(\"./data/test\", transform=test_transforms)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=MODEL_CONFIG[\"batch_size\"], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=MODEL_CONFIG[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    model = NeuralNetwork(\n",
    "        input_size=MODEL_ARCHITECTURE_CONFIG[\"input_size\"],\n",
    "        hidden_sizes=MODEL_ARCHITECTURE_CONFIG[\"hidden_sizes\"],\n",
    "        num_classes=MODEL_ARCHITECTURE_CONFIG[\"num_classes\"],\n",
    "        dropout_rates=MODEL_ARCHITECTURE_CONFIG[\"dropout_rates\"]\n",
    "    ).to(device)\n",
    "    \n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=MODEL_CONFIG[\"learning_rate\"])\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=MODEL_CONFIG[\"learning_rate\"], weight_decay=MODEL_CONFIG.get(\"weight_decay\", 0.01))\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min',           # minimize the loss\n",
    "        factor=0.5,           # reduce LR by half\n",
    "        patience=5,           # wait 5 epochs before reducing\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    es_config = hp_config.get(\"early_stopping\", {})\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=es_config.get(\"patience\", 7),\n",
    "        min_delta=es_config.get(\"min_delta\", 0.001),\n",
    "        mode=es_config.get(\"mode\", \"min\"),\n",
    "        restore_best_weights=es_config.get(\"restore_best_weights\", True)\n",
    "    )\n",
    " \n",
    "    start_time = time.time()\n",
    "    history = train_model(      \n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=MODEL_CONFIG[\"epochs\"],\n",
    "        test_loader=test_loader,\n",
    "        verbose=True,\n",
    "        scheduler=scheduler,\n",
    "        early_stopping=early_stopping\n",
    "    )\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    final_train_metrics = {\n",
    "        'loss': history['train_losses'][-1],\n",
    "        'accuracy': history['train_accuracies'][-1],\n",
    "        'f1_macro': history['train_f1_macro'][-1],\n",
    "        'precision_macro': history['train_precision_macro'][-1],\n",
    "        'recall_macro': history['train_recall_macro'][-1],\n",
    "    }\n",
    "    \n",
    "    final_test_metrics = {\n",
    "        'loss': history['test_losses'][-1],\n",
    "        'accuracy': history['test_accuracies'][-1],\n",
    "        'f1_macro': history['test_f1_macro'][-1],\n",
    "        'precision_macro': history['test_precision_macro'][-1],\n",
    "        'recall_macro': history['test_recall_macro'][-1],\n",
    "    }\n",
    "    \n",
    "    plot_loss_curves(history, save_path=os.path.join(run_dir, \"loss_curve.png\"), show=False)\n",
    "    plot_accuracy_curves(history, save_path=os.path.join(run_dir, \"accuracy_curve.png\"), show=False)\n",
    "    plot_f1_curves(history, save_path=os.path.join(run_dir, \"f1_curve.png\"), show=False)\n",
    "    plot_precision_curves(history, save_path=os.path.join(run_dir, \"precision_curve.png\"), show=False)\n",
    "    plot_recall_curves(history, save_path=os.path.join(run_dir, \"recall_curve.png\"), show=False)\n",
    "    \n",
    "    test_eval = evaluate_model(model, test_loader, criterion, device)\n",
    "    plot_confusion_matrix(\n",
    "        test_eval['y_true'], \n",
    "        test_eval['y_pred'],\n",
    "        class_names=train_dataset.classes,\n",
    "        save_path=os.path.join(run_dir, \"confusion_matrix.png\"),\n",
    "        show=False\n",
    "    )\n",
    "    \n",
    "    model_path = save_model(model, run_dir, final_test_metrics, model_name=\"model.pth\")\n",
    "    \n",
    "    full_config = {\n",
    "        'hyperparameters': hp_config,\n",
    "        'model_config': MODEL_CONFIG,\n",
    "        'architecture_config': MODEL_ARCHITECTURE_CONFIG,\n",
    "        'transform_config': TRANSFORM_CONFIG,\n",
    "    }\n",
    "    save_config_to_json(full_config, run_dir, filename=\"config.json\")\n",
    "    \n",
    "    run_data = {\n",
    "        'run_id': run_id,\n",
    "        'run_dir': run_dir,  # Include directory path\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_time': total_time,\n",
    "        'hyperparameters': hp_config,\n",
    "        'config': {\n",
    "            'input_size': MODEL_ARCHITECTURE_CONFIG[\"input_size\"],\n",
    "            'num_classes': MODEL_ARCHITECTURE_CONFIG[\"num_classes\"],\n",
    "        },\n",
    "        'train_metrics': final_train_metrics,\n",
    "        'test_metrics': final_test_metrics,\n",
    "        \"notes\": hp_config.get(\"notes\", \"\")\n",
    "    }\n",
    "    save_run_to_csv(run_data, csv_path=\"model_runs.csv\")\n",
    "    \n",
    "    if final_test_metrics['accuracy'] > best_test_accuracy:\n",
    "        best_test_accuracy = final_test_metrics['accuracy']\n",
    "        best_run_id = run_id\n",
    "        best_run_dir = run_dir\n",
    "        # Optionally create a symlink or copy to \"best_model\" directory\n",
    "        print(f\"\\n*** New best model! Test Accuracy: {best_test_accuracy:.4f} ***\\n\")\n",
    "    \n",
    "    print(f\"Run {run_id} completed in {total_time:.2f} seconds\")\n",
    "    print(f\"Test Accuracy: {final_test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"All artifacts saved to: {run_dir}\\n\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Hyperparameter search complete!\")\n",
    "print(f\"Best run: {best_run_id}\")\n",
    "print(f\"Best run directory: {best_run_dir}\")\n",
    "print(f\"Best test accuracy: {best_test_accuracy:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a879986",
   "metadata": {},
   "source": [
    "<img src=\"./images/biasvariance.png\" alt=\"Machine Learning Recipe\" width=\"800\">\n",
    "\n",
    "\n",
    "<p> I got high training data but low bias... I need to increase my model size? L1 is not typically used in NN,</p>\n",
    "\n",
    "We donot use dropout regularization in test\n",
    "\n",
    "<li>\n",
    "<ul>Data Augmentation</ul>\n",
    "<ul>Early Stopping</ul>\n",
    "<ul>Normalization of train sets</ul>\n",
    "</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221f2d86",
   "metadata": {},
   "source": [
    "# Explanation of the developed code\n",
    "\n",
    "Explain the essential functions/classes you developed in your code.\n",
    "\n",
    "Explain how to run your code if you are using the “command prompt” or any environment other than VS Code and Jupyter. \n",
    "\n",
    "<div style = \"color:blue\">\n",
    "- WasteClassificationDataset Class\n",
    "This class was was created by inheriting the Dataset Class provided by PyTorch. Three required methods were created which include _init__, __len__ and __getitem__ which are required by PyTorch Dataset interface.\n",
    "\n",
    "The images were organized by class folders(paper, metal, glass, plastic), which made it easier to create class labels for the dataset.\n",
    "\n",
    "This could've been done using base Python as well, but PyTorch provides additional helpful tooks like Automatic Differentiation for back propogation, optimizers etc which is why we need PyTorch to convert the dataset into tensors.\n",
    "\n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd9b46f",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization \n",
    "\n",
    "Optimize your “learning rate” and “number of layers\n",
    "What is the effect of “optimization” on “learning rate” and “number of layers”?  (indicate starting values and the optimized values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "How did you optimize your hyperparameters?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What are the optimization results (show the numerical values and state how you improved them)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cd4fd1",
   "metadata": {},
   "source": [
    "# Discuss the Performance of Your Model:\n",
    "\n",
    "- Confusion matrix \n",
    "\n",
    "What is the metric selected to evaluate your model?\n",
    "\n",
    "\n",
    "What is the best metric value obtained? \n",
    "\n",
    "\n",
    "What kind of error(s) do you have in your model? Discuss your errors, state what they mean. \n",
    "\n",
    "\n",
    "Due to the errors stated above, what shall be done to improve your model’s performance?\n",
    "\n",
    "\n",
    "Which techniques have you applied to improve your model’s performance? \n",
    "\n",
    "\n",
    "Do you have overfitting or underfitting/overfitting in your model? Which processes did you apply to prevent underfitting/overfitting? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04791bf",
   "metadata": {},
   "source": [
    "# Error analysis and misclassified examples\n",
    "\n",
    "Please discuss the error analysis you performed in your project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b97f58",
   "metadata": {},
   "source": [
    "# Deep Learning Strategies\n",
    "\n",
    "What are the Deep Learning Strategies you performed in this project? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "If they exist, why did you select these strategies to apply? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c7adde",
   "metadata": {},
   "source": [
    "# Discussion and Conclusion\n",
    "\n",
    "Discuss your results and draw conclusions from your work. State what you learned in this project. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7911295",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fbddfec",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
